# -*- coding: utf-8 -*-
"""Implemented_SVR

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18Rc2PlpfIlNHnuH08fs1mYbel8_akGxV
"""

import numpy as np

import pandas as pd

from sklearn.svm import SVR

from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(name = fn, length = len(uploaded[fn])))

import io

df = pd.read_csv(io.StringIO(uploaded['SVM_DATA.csv'].decode('utf-8')))

df = np.array(df)

data = []

for i in range(0, len(df), 2):
    row = []
    temp1 = df[i][0].split()
    temp2 = df[i+1][0].split()
    for t in temp1:
        row.append(float(t))
    for t in temp2:
        row.append(float(t))
        
    row = np.array(row)
    data.append(row)

data = np.array(data)

data.shape

data -= np.mean(data, axis=0)
data /= np.std(data, axis=0)

import matplotlib.pyplot as plt

import cvxopt
from cvxopt import matrix
from cvxopt import solvers

def Kernel(name, x, y, gamma):
  if name == 'linear':
    return np.dot(x,y)
  
  elif name == 'rbf':
    #gamma = 1/(np.var(x) + np.var(y))
    return np.exp(-gamma*np.matmul((x-y).transpose(),(x-y)))

  elif name == 'poly':
    return (np.dot(x,y)+1)**(2) # Quadratic Kernel
  
  else:
    print("Please Enter - 'linear' or 'poly' or 'rbf' ")

def predictor(X_train,x,Alpha,B,s, gamma):
  ans = 0
  N = int(len(Alpha)/2)
  for i in range(N):
    ans += (Alpha[i]-Alpha[N+i])*Kernel(s,x,X_train[i], gamma)
  return ans

# K-Fold
k_fold_size = 101
no_of_fold = int(len(data)/k_fold_size)
count = 0
MSE = 0
for i in range(no_of_fold):

    count+=1
    print("Validation No : ",count," -------------------------------------------------")


    X_test = np.array([data[j][:-1] for j in range(i*k_fold_size,(i+1)*k_fold_size)])
    Y_test = np.array([data[j][-1] for j in range(i*k_fold_size,(i+1)*k_fold_size)])
    
    X_train = []
    Y_train = []
    for j in range(len(data)):
        if (j<i*k_fold_size) or (j>(i+1)*k_fold_size):
            X_train.append(data[j][:-1])
            Y_train.append(data[j][-1])
    X_train = np.array(X_train)
    Y_train = np.array(Y_train)
    

    # Hyper-parameters
    epsilon = 0.5
    C = 100
    ker = 'linear'
    gamma = 0.1

    # Optimization COnversion
    K = []
    for x1 in X_train:
      row = []
      for x2 in X_train:
        row.append(Kernel(ker,x1,x2,gamma))
      K.append(row)

    # Sub matrices
    N = len(X_train)
    K = matrix(np.array(K),tc='d')
    I = matrix(np.eye(N),tc='d')
    O = matrix(np.zeros([N,N]),tc='d')
    Ones_N_1 = matrix(np.ones([N,1]),tc='d')
    Zeros_N_1 = matrix(np.zeros([N,1]),tc='d')
    y = matrix(Y_train,tc='d')


    # Transformation into Standard Form
    P = (1/2)*matrix([[K,-K],[-K,K]],tc='d')
    q = matrix([(y-epsilon*Ones_N_1), -(y+epsilon*Ones_N_1)],tc='d')
    G = matrix([[-I,O,I,O],[O,-I,O,I]],tc='d')
    h = matrix([Zeros_N_1,Zeros_N_1,C*Ones_N_1,C*Ones_N_1],tc='d')
    A = matrix([[matrix(1,(1,N),tc='d')],[matrix(-1, (1,N),tc='d')]],tc='d')
    b = matrix([0],tc='d')
    
    # Dimension Testing

    # Matrices = [P,q,G,h,A,b,K,I,O,Ones_N_1,Zeros_N_1,y]
    # for m in Matrices:
    #   print(m.size)
    
    # print("X_train ",len(X_train))
    # print("Y_train ",len(Y_train))
    # print("X_test ",len(X_test))
    # print("Y_test ",len(Y_test))
    # print("")

    # Solving Optimization Problem
    sol = solvers.qp(P,q,G,h,A,b)

    x = sol['x']
    B = matrix(1,(1,N),tc='d')*(matrix([[K],[-K]],tc='d')*x + matrix([(y-epsilon*Ones_N_1)]))
    B /= N


    #***********************************************************************************************************************************
    # Regression Using Implemented Model
    # Prediction Stage
    Y_predicted = []
    sq_err = 0
    for t,y in zip(X_test,Y_test):
      fx = predictor(X_train, t, x, B, ker,gamma)
      Y_predicted.append(fx)
      
      if(abs(y-fx) > epsilon):
        sq_err += (abs(y-fx)-epsilon)**2
    
    Y_predicted = np.array(Y_predicted)
    MSE += sq_err
    
    # Data Visualisation
    plt.scatter(X_test[:,0], Y_predicted, color = 'm',label = "Predicted")
    plt.scatter(X_test[:,0], Y_test, color = 'g', label = "Actual")
    plt.xlabel("1st Column of X")
    plt.ylabel("Median Val")
    plt.title("Validation Fold No : %d\nKernel=%s, C=%d, epsilon=%f, Gamma=%f\n MSE=%f"%(i+1,ker,C,epsilon,gamma,sq_err))
    plt.legend()
    plt.show()

    #*************************************************************************************************************************************

    ######################################################################################################################################
    
    # Regression Model Using Sklearn

    # svr_rbf = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)
    # svr_lin = SVR(kernel='linear', C=100, gamma='auto')
    # svr_poly = SVR(kernel='poly',C=100, degree=2)
    
    # svr_lin.fit(X_train, Y_train)
    # Y_predicted = svr_lin.predict(X_test)
    # score = svr_lin.score(X_test,Y_test)
    
    # # Accuracy Test
    # sq_err = 0
    # for y1,y2 in zip(Y_test,Y_predicted):
    #     if(abs(y1-y2) > epsilon):
    #         sq_err += (abs(y1-y2)-epsilon)**2
    # #print("Error from ",i,"th validation : ",sq_err)
    # MSE += sq_err

    # # Data Visualisation
    # plt.scatter(X_test[:,0], Y_predicted, color = 'm',label = "Predicted")
    # plt.scatter(X_test[:,0], Y_test, color = 'g', label = "Actual")
    # plt.xlabel("1st Column of X")
    # plt.ylabel("Median Val")
    # plt.title("Validation Fold No : %d\nKernel=Linear, C=100, epsilon=0.5, Gamma=0.1\nScore=%f, MSE=%f"%(i+1,score,sq_err))
    # plt.legend()
    # plt.show()

    #########################################################################################################################################

print("Net Error = ",(MSE/no_of_fold))

